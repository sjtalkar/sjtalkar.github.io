{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.0** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-text-mining/resources/d9pwm) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLTK\n",
    "\n",
    "In part 1 of this assignment you will use nltk to explore the Herman Melville novel Moby Dick. Then in part 2 you will create a spelling recommender function that uses nltk to find words similar to the misspelling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Analyzing Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# If you would like to work with the raw text you can use 'moby_raw'\n",
    "with open('moby.txt', 'r') as f:\n",
    "    moby_raw = f.read()\n",
    "    \n",
    "# If you would like to work with the novel in nltk.Text format you can use 'text1'\n",
    "moby_tokens = nltk.word_tokenize(moby_raw)\n",
    "text1 = nltk.Text(moby_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "How many tokens (words and punctuation symbols) are in text1?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255038"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_one():\n",
    "    \n",
    "    return len(nltk.word_tokenize(moby_raw)) # or alternatively len(text1)\n",
    "\n",
    "example_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255038"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(moby_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "How many unique tokens (unique words and punctuation) does text1 have?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20742"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_two():\n",
    "    \n",
    "    return len(set(nltk.word_tokenize(moby_raw))) # or alternatively len(set(text1))\n",
    "\n",
    "example_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "After lemmatizing the verbs, how many unique tokens does text1 have?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16887"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def example_three():\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]\n",
    "\n",
    "    return len(set(lemmatized))\n",
    "\n",
    "example_three()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(w,'v') for w in moby_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'be', 'once', 'lose', 'but', 'now', 'have', 'be', 'find', 'garden', ';']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = ['I', 'was', 'once', 'lost', 'but', 'now', 'have', 'been' ,'found', 'gardening', ';']\n",
    "[lemmatizer.lemmatize(w,'v') for w in my_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'was',\n",
       " 'once',\n",
       " 'lost',\n",
       " 'but',\n",
       " 'now',\n",
       " 'have',\n",
       " 'been',\n",
       " 'found',\n",
       " 'gardening',\n",
       " ';']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = ['I', 'was', 'once', 'lost', 'but', 'now', 'have', 'been' ,'found', 'gardening', ';']\n",
    "[lemmatizer.lemmatize(w,'s') for w in my_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08132905684643073"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_one():\n",
    "    '''\n",
    "    Returns: The ratio between unique tokens and all tokens\n",
    "    '''\n",
    "    \n",
    "    total_tokens = len(moby_tokens)\n",
    "    \n",
    "    unique_tokens = len(set(moby_tokens))\n",
    "    \n",
    "    return unique_tokens/total_tokens\n",
    "\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens = 255038 Length of unique tokens = 20742\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total tokens = {len(moby_tokens)} Length of unique tokens = {len(set(moby_tokens))}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "What percentage of tokens is 'whale'or 'Whale'?\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41248755087477157"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_two():\n",
    "    '''\n",
    "        Alternative 1: Uses list comprehension to get count through exact match\n",
    "    '''\n",
    "    \n",
    "    whale_tokens = [token for token in moby_tokens if  token ==   \"Whale\" or token == 'whale'] \n",
    "    \n",
    "    return len(whale_tokens) / len (moby_tokens) * 100\n",
    "\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of working with lists, create a dataframe for all the tasks\n",
    "def getWordFreqDF():\n",
    "    '''\n",
    "        This function sets up a dataframe that will be used by all functions below\n",
    "        The FreqDist of the tokens dictionary is converted to a dataframe\n",
    "        The length of the tokens and a flag to indicate if the word is alphabetic or punctuation is added\n",
    "        The returned dataframe is sorted by frequency count\n",
    "    '''\n",
    "   \n",
    "    from nltk.probability import FreqDist\n",
    "    moby_freq_dist = FreqDist(moby_tokens)\n",
    "\n",
    "    word_df = pd.DataFrame.from_dict( moby_freq_dist, orient='index' )\n",
    "    word_df.reset_index(inplace=True)\n",
    "    word_df.columns = ['word_token', 'word_freq']\n",
    "    word_df['word_len'] = word_df['word_token'].apply(len)\n",
    "    word_df['word_isalpha'] = word_df['word_token'].apply(lambda x: x.isalpha())\n",
    "    word_df.sort_values(by=['word_freq'], ascending=False, inplace=True)\n",
    "    return word_df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_text_df = getWordFreqDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_token</th>\n",
       "      <th>word_freq</th>\n",
       "      <th>word_len</th>\n",
       "      <th>word_isalpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>,</td>\n",
       "      <td>19204</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>the</td>\n",
       "      <td>13715</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.</td>\n",
       "      <td>7306</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>of</td>\n",
       "      <td>6513</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>and</td>\n",
       "      <td>6010</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13310</th>\n",
       "      <td>passionateness</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13311</th>\n",
       "      <td>ireful</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>aggrieved</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5155</th>\n",
       "      <td>bruised</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20741</th>\n",
       "      <td>orphan</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20742 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word_token  word_freq  word_len  word_isalpha\n",
       "26                  ,      19204         1         False\n",
       "50                the      13715         3          True\n",
       "9                   .       7306         1         False\n",
       "53                 of       6513         2          True\n",
       "29                and       6010         3          True\n",
       "...               ...        ...       ...           ...\n",
       "13310  passionateness          1        14          True\n",
       "13311          ireful          1         6          True\n",
       "13312       aggrieved          1         9          True\n",
       "5155          bruised          1         7          True\n",
       "20741          orphan          1         6          True\n",
       "\n",
       "[20742 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moby_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41248755087477157"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_two():\n",
    "    '''\n",
    "        Add the frequencies of the words whale and Whale\n",
    "    '''\n",
    "   \n",
    "    moby_freq_dist = FreqDist(moby_tokens)\n",
    "    whale_freq = moby_freq_dist['whale'] +  moby_freq_dist['Whale'] \n",
    "    return (whale_freq)/moby_text_df['word_freq'].sum() * 100\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004124875508747716"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using dataframe\n",
    "def answer_two():\n",
    "    return moby_text_df[moby_text_df['word_token'].isin(['Whale', 'whale'])]['word_freq'].sum()/ len(moby_tokens) \n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?\n",
    "\n",
    "*This function should return a list of 20 tuples where each tuple is of the form `(token, frequency)`. The list should be sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26       (,, 19204)\n",
       "50     (the, 13715)\n",
       "9         (., 7306)\n",
       "53       (of, 6513)\n",
       "29      (and, 6010)\n",
       "12        (a, 4545)\n",
       "16       (to, 4515)\n",
       "31        (;, 4173)\n",
       "24       (in, 3908)\n",
       "94     (that, 2978)\n",
       "40      (his, 2459)\n",
       "59       (it, 2196)\n",
       "32        (I, 2113)\n",
       "275       (!, 1767)\n",
       "76       (is, 1722)\n",
       "22       (--, 1713)\n",
       "44     (with, 1659)\n",
       "174      (he, 1658)\n",
       "37      (was, 1639)\n",
       "205      (as, 1620)\n",
       "Name: token_freq_combine, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using dataframe\n",
    "def answer_three():\n",
    "    '''\n",
    "       Since the dataframe is already sorted, return the top 20. \n",
    "       Note the usage of zip to combine columns of the dataframe into a list of tuples\n",
    "    '''\n",
    "   \n",
    "    top_20 =  moby_text_df[:20][['word_token', 'word_freq']]\n",
    "    top_20['token_freq_combine'] = list(zip(top_20['word_token'], top_20['word_freq'] ))\n",
    "    return top_20['token_freq_combine']\n",
    "\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7306),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978),\n",
       " ('his', 2459),\n",
       " ('it', 2196),\n",
       " ('I', 2113),\n",
       " ('!', 1767),\n",
       " ('is', 1722),\n",
       " ('--', 1713),\n",
       " ('with', 1659),\n",
       " ('he', 1658),\n",
       " ('was', 1639),\n",
       " ('as', 1620)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three():\n",
    "    moby_freq_dist = FreqDist(moby_tokens)\n",
    "   \n",
    "    # sort the dictionary\n",
    "    moby_freq_sorted = {k: v for k, v in sorted(moby_freq_dist.items(), key=lambda item: item[1], reverse=True)}\n",
    "   \n",
    "    return [(k, v) for i, (k, v) in enumerate(moby_freq_sorted.items())][:20]\n",
    "\n",
    "\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7306),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978),\n",
       " ('his', 2459),\n",
       " ('it', 2196),\n",
       " ('I', 2113),\n",
       " ('!', 1767),\n",
       " ('is', 1722),\n",
       " ('--', 1713),\n",
       " ('with', 1659),\n",
       " ('he', 1658),\n",
       " ('was', 1639),\n",
       " ('as', 1620)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist_tokens = FreqDist(moby_tokens)\n",
    "freq_dist_tokens.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What tokens have a length of greater than 5 and frequency of more than 150?\n",
    "\n",
    "*This function should return an alphabetically sorted list of the tokens that match the above constraints. To sort your list, use `sorted()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1899     Captain\n",
       "6220      Pequod\n",
       "3625    Queequeg\n",
       "7421    Starbuck\n",
       "88        almost\n",
       "301       before\n",
       "1367     himself\n",
       "872       little\n",
       "886       seemed\n",
       "731       should\n",
       "1080      though\n",
       "83       through\n",
       "173       whales\n",
       "639      without\n",
       "Name: word_token, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using dataframe\n",
    "def answer_four():\n",
    "    \n",
    "    to_return_df = moby_text_df[(moby_text_df['word_freq'] > 150) & (moby_text_df['word_len'] > 5)]\n",
    "    to_return_df = to_return_df.sort_values(by=['word_token'], ascending=True)\n",
    "   \n",
    "    return to_return_df['word_token']\n",
    "\n",
    "answer_four()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Captain',\n",
       " 'Pequod',\n",
       " 'Queequeg',\n",
       " 'Starbuck',\n",
       " 'almost',\n",
       " 'before',\n",
       " 'himself',\n",
       " 'little',\n",
       " 'seemed',\n",
       " 'should',\n",
       " 'though',\n",
       " 'through',\n",
       " 'whales',\n",
       " 'without']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_four():\n",
    "    \n",
    "    to_return = sorted(\n",
    "                    [item[0] for item in freq_dist_tokens.items()\n",
    "                     if item[1] > 150 and len(item[0]) > 5])\n",
    "    \n",
    "    return to_return\n",
    "\n",
    "answer_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Find the longest word in text1 and that word's length.\n",
    "\n",
    "*This function should return a tuple `(longest_word, length)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"twelve-o'clock-at-night\", 23)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_five():\n",
    "    '''\n",
    "    Since the length of each token is a column in the dataframe, find the index of the row containing \n",
    "    the max length to access the word\n",
    "    '''\n",
    "   \n",
    "    max_idx = moby_text_df['word_len'].idxmax()\n",
    "    return (moby_text_df.loc[max_idx, 'word_token'], moby_text_df.loc[max_idx, 'word_len'])\n",
    "\n",
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What unique words have a frequency of more than 2000? What is their frequency?\n",
    "\n",
    "\"Hint:  you may want to use `isalpha()` to check if the token is a word and not punctuation.\"\n",
    "\n",
    "*This function should return a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13715, 'the'),\n",
       " (6513, 'of'),\n",
       " (6010, 'and'),\n",
       " (4545, 'a'),\n",
       " (4515, 'to'),\n",
       " (3908, 'in'),\n",
       " (2978, 'that'),\n",
       " (2459, 'his'),\n",
       " (2196, 'it'),\n",
       " (2113, 'I')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_six():\n",
    "    \n",
    "    to_return_df = moby_text_df [moby_text_df['word_freq'] > 2000]\n",
    "    \n",
    "    to_return_df = to_return_df[to_return_df['word_token'].apply(lambda x: x.isalpha())]\n",
    "    to_return_df.sort_values(by=['word_freq'], ascending = False, inplace=True)\n",
    "    return list(zip(to_return_df['word_freq'],to_return_df['word_token']))\n",
    "  \n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "What is the average number of tokens per sentence?\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.886926512383273"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_seven():\n",
    "    \n",
    "    from nltk import sent_tokenize\n",
    "    from nltk import word_tokenize\n",
    "\n",
    "    moby_sentences = sent_tokenize(moby_raw)\n",
    "    moby_word_tokens = word_tokenize(moby_raw)\n",
    "\n",
    "    \n",
    "    return len(moby_word_tokens)/len(moby_sentences)\n",
    "\n",
    "answer_seven()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "What are the 5 most frequent parts of speech in this text? What is their frequency?\n",
    "\n",
    "*This function should return a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def answer_eight():\n",
    "    \n",
    "    moby_onlywords_df = moby_text_df[moby_text_df['word_isalpha']].copy()\n",
    "    moby_onlywords_df['pos_tag'] = nltk.pos_tag(moby_onlywords_df['word_token'])\n",
    "    moby_onlywords_df[['pos', 'tag']] = pd.DataFrame(moby_onlywords_df['pos_tag'].tolist(), index=moby_onlywords_df.index)   \n",
    "    \n",
    "    #Note you CANNOT count the rows for the freq you have to SUM the freq\n",
    "    tag_df = moby_onlywords_df.groupby(['tag']).agg(tag_freq=('word_freq', 'sum'))\n",
    "    tag_df = tag_df.reset_index()\n",
    "    tag_df = tag_df.sort_values(by=['tag_freq'], ascending=False)\n",
    "    return  list(zip(tag_df['tag'], tag_df['tag_freq']))[:5]\n",
    "    \n",
    "df = answer_eight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DT', 28037), ('IN', 27240), ('NN', 26565), ('JJ', 20311), ('RB', 14315)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Spelling Recommender\n",
    "\n",
    "For this part of the assignment you will create three different spelling recommenders, that each take a list of misspelled words and recommends a correctly spelled word for every word in the list.\n",
    "\n",
    "For every misspelled word, the recommender should find find the word in `correct_spellings` that has the shortest distance*, and starts with the same letter as the misspelled word, and return that word as a recommendation.\n",
    "\n",
    "*Each of the three different recommenders will use a different distance measure (outlined below).\n",
    "\n",
    "Each of the recommenders should provide recommendations for the three default words provided: `['cormulent', 'incendenece', 'validrate']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics.distance import edit_distance, jaccard_distance\n",
    "    \n",
    "from nltk.util import ngrams, trigrams\n",
    "\n",
    "correct_spellings = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe of all words\n",
    "correct_spellings_df = pd.DataFrame(correct_spellings, columns=['correct_word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the trigrams of the two words.**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacDist(col_word, entry_word, gramno):\n",
    "    '''\n",
    "       Utility function to find Jaccard similarity between sets of ngrams\n",
    "    '''\n",
    "   \n",
    "    return jaccard_distance(set(ngrams(col_word, gramno)), set(ngrams(entry_word, gramno)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpulent', 'indecence', 'validate']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    \n",
    "    correct_word_list = []\n",
    "    for entry in entries:\n",
    "        first_char = entry[0]\n",
    "        temp_df = correct_spellings_df[correct_spellings_df['correct_word'].str.startswith(first_char)].copy()\n",
    "        temp_df['jac_dist'] = temp_df['correct_word'].apply(jacDist, args=(entry, 3))\n",
    "                                    \n",
    "        correct_word_list.append(temp_df.loc[temp_df['jac_dist'].idxmin()]['correct_word'])\n",
    "    return correct_word_list\n",
    "    \n",
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the 4-grams of the two words.**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cormus', 'incendiary', 'valid']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_ten(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    correct_word_list = []\n",
    "    for entry in entries:\n",
    "        first_char = entry[0]\n",
    "        temp_df = correct_spellings_df[correct_spellings_df['correct_word'].str.startswith(first_char)].copy()\n",
    "        temp_df['jac_dist'] = temp_df['correct_word'].apply(jacDist, args=(entry, 4))\n",
    "                                    \n",
    "        correct_word_list.append(temp_df.loc[temp_df['jac_dist'].idxmin()]['correct_word'])\n",
    "    return correct_word_list\n",
    "  \n",
    "    \n",
    "answer_ten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Edit distance on the two words with transpositions.](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('some', 'simi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpulent', 'intendence', 'validate']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    correct_word_list = []\n",
    "    for entry in entries:\n",
    "        first_char = entry[0]\n",
    "        temp_df = correct_spellings_df[correct_spellings_df['correct_word'].str.startswith(first_char)].copy()\n",
    "        temp_df['edit_dist'] = temp_df['correct_word'].apply(lambda x: edit_distance(x, entry))\n",
    "                                    \n",
    "        correct_word_list.append(temp_df.loc[temp_df['edit_dist'].idxmin()]['correct_word'])\n",
    "    return correct_word_list\n",
    "    \n",
    "answer_eleven()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "r35En",
   "launcher_item_id": "tCVfW",
   "part_id": "NTVgL"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
